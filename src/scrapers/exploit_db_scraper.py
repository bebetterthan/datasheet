"""
Exploit-DB scraper - scrapes exploits from Exploit Database.
Uses GitLab mirror to avoid Cloudflare protection.
Target: https://gitlab.com/exploit-database/exploitdb
"""

import os
import re
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin, urlencode
import csv
import io

from bs4 import BeautifulSoup

from .base_scraper import BaseScraper, ScrapedItem
from ..utils.logger import get_logger
from ..utils.github_helper import GitHubHelper, GitHubAPIError

logger = get_logger(__name__)


class ExploitDBScraper(BaseScraper):
    """
    Scraper for Exploit-DB exploit database.
    Uses GitLab mirror to avoid Cloudflare protection.
    Extracts exploits, PoCs, and vulnerability information.
    """
    
    SOURCE_NAME = "exploit_db"
    BASE_URL = "https://www.exploit-db.com"
    
    # GitLab mirror URLs (no Cloudflare!)
    GITLAB_RAW = "https://gitlab.com/exploit-database/exploitdb/-/raw/main"
    GITLAB_CSV = f"{GITLAB_RAW}/files_exploits.csv"
    
    # Exploit types
    EXPLOIT_TYPES = {
        "webapps": "web_security",
        "remote": "exploitation/remote",
        "local": "exploitation/local",
        "dos": "exploitation/dos",
        "shellcode": "exploitation/shellcode",
        "papers": "security/research",
    }
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._discovered_urls: List[Dict] = []
        self._exploits_csv: Optional[List[Dict]] = None
    
    async def discover_urls(self) -> List[str]:
        """Discover exploit URLs from Exploit-DB GitLab mirror."""
        if self._discovered_urls:
            return [e['file'] for e in self._discovered_urls]
        
        logger.info("Discovering Exploit-DB exploits from GitLab mirror...")
        
        # Load exploits CSV from GitLab
        await self._load_exploits_csv()
        
        if not self._exploits_csv:
            logger.warning("Could not load exploits database")
            return []
        
        # Get filter configuration
        exploit_types = ["webapps", "remote", "local", "dos"]
        if self.source_config and hasattr(self.source_config, 'extra_params'):
            extra = self.source_config.extra_params or {}
            if 'types' in extra:
                exploit_types = extra['types']
        
        years_back = 3
        if self.source_config and hasattr(self.source_config, 'extra_params'):
            extra = self.source_config.extra_params or {}
            if 'years_back' in extra:
                years_back = extra['years_back']
        
        cutoff_date = datetime.now() - timedelta(days=years_back * 365)
        
        # Filter exploits
        filtered = []
        for exploit in self._exploits_csv:
            # Filter by type
            platform = exploit.get('platform', '').lower()
            e_type = exploit.get('type', '').lower()
            
            type_match = any(t in platform or t in e_type for t in exploit_types)
            
            if not type_match:
                continue
            
            # Filter by date
            date_str = exploit.get('date', '')
            try:
                if date_str:
                    exploit_date = datetime.strptime(date_str, '%Y-%m-%d')
                    if exploit_date < cutoff_date:
                        continue
            except ValueError:
                pass
            
            filtered.append(exploit)
        
        self._discovered_urls = filtered
        logger.info(f"Discovered {len(filtered)} exploits matching criteria")
        
        return [e['file'] for e in filtered if e.get('file')]
    
    async def _load_exploits_csv(self):
        """Load exploits database from GitLab CSV."""
        try:
            logger.info("Downloading exploits CSV from GitLab mirror...")
            csv_content = await self.fetch_page(self.GITLAB_CSV)
            
            if not csv_content:
                logger.error("Could not download exploits CSV")
                return
            
            # Parse CSV
            reader = csv.DictReader(io.StringIO(csv_content))
            self._exploits_csv = list(reader)
            logger.info(f"Loaded {len(self._exploits_csv)} exploits from database")
            
        except Exception as e:
            logger.error(f"Error loading exploits CSV: {e}")
    
    async def scrape_page(self, file_path: str) -> Optional[ScrapedItem]:
        """Scrape a single exploit from GitLab mirror."""
        # Find exploit info from CSV
        exploit_info = None
        for e in self._discovered_urls:
            if e.get('file') == file_path:
                exploit_info = e
                break
        
        if not exploit_info:
            return None
        
        # Build URL to raw exploit file (file_path already includes 'exploits/')
        url = f"{self.GITLAB_RAW}/{file_path}"
        
        try:
            exploit_code = await self.fetch_page(url)
            if not exploit_code:
                return None
            
            # Extract metadata from CSV record
            title = exploit_info.get('description', f"Exploit: {file_path}")
            edb_id = exploit_info.get('id', '')
            cve = exploit_info.get('codes', '')  # May contain CVE;OSVDB format
            platform = exploit_info.get('platform', '')
            exploit_type = exploit_info.get('type', '')
            author = exploit_info.get('author', '')
            date = exploit_info.get('date_published', '')
            
            # Detect language
            language = self._detect_language(exploit_code)
            
            # Build content
            content_parts = [
                f"# {title}",
                f"\nExploit-DB ID: {edb_id}" if edb_id else "",
                f"CVE: {cve}" if cve else "",
                f"Platform: {platform}" if platform else "",
                f"Type: {exploit_type}" if exploit_type else "",
                f"Author: {author}" if author else "",
                f"Date: {date}" if date else "",
                f"\n## Exploit Code\n",
            ]
            
            content = '\n'.join(p for p in content_parts if p)
            
            code_blocks = [{
                'code': exploit_code,
                'language': language,
                'type': 'exploit',
            }]
            
            # Determine category
            category = self._determine_category_from_info(exploit_info, title)
            
            return ScrapedItem(
                url=f"https://www.exploit-db.com/exploits/{edb_id}",
                title=title,
                content=content,
                code_blocks=code_blocks,
                headers=[],
                metadata={
                    'category': category,
                    'difficulty': 'advanced',
                    'source_type': 'exploit_db',
                    'edb_id': edb_id,
                    'cve': cve,
                    'platform': platform,
                    'exploit_type': exploit_type,
                    'author': author,
                }
            )
        
        except Exception as e:
            logger.error(f"Error scraping exploit {file_path}: {e}")
            return None
    
    def _determine_category_from_info(self, info: Dict, title: str) -> str:
        """Determine category from exploit info."""
        platform = info.get('platform', '').lower()
        e_type = info.get('type', '').lower()
        title_lower = title.lower()
        
        # Check type mapping
        for type_key, category in self.EXPLOIT_TYPES.items():
            if type_key in e_type or type_key in platform:
                return category
        
        # Check for specific vulnerabilities
        if any(kw in title_lower for kw in ['sql injection', 'sqli']):
            return 'web_security/sql_injection'
        if any(kw in title_lower for kw in ['xss', 'cross-site']):
            return 'web_security/xss'
        if any(kw in title_lower for kw in ['rce', 'remote code execution']):
            return 'exploitation/rce'
        if any(kw in title_lower for kw in ['buffer overflow', 'bof']):
            return 'exploitation/buffer_overflow'
        if any(kw in title_lower for kw in ['privilege escalation', 'privesc']):
            return 'privilege_escalation'
        
        return 'exploitation'
    
    def _detect_language(self, code: str) -> str:
        """Detect exploit language."""
        code_lower = code.lower()[:500]  # Check first 500 chars
        
        if '#!/usr/bin/python' in code_lower or 'import ' in code_lower:
            return 'python'
        if '#!/bin/bash' in code_lower or '#!/bin/sh' in code_lower:
            return 'bash'
        if '<?php' in code_lower:
            return 'php'
        if '#include' in code_lower:
            return 'c'
        if 'require ' in code_lower and 'end' in code_lower:
            return 'ruby'
        if 'use strict' in code_lower or 'my $' in code_lower:
            return 'perl'
        if 'powershell' in code_lower or '$env:' in code_lower:
            return 'powershell'
        
        return ''
    
    def _determine_category(self, metadata: Dict, title: str) -> str:
        """Determine category from metadata and title."""
        exploit_type = metadata.get('type', '').lower()
        platform = metadata.get('platform', '').lower()
        title_lower = title.lower()
        
        # Check type mapping
        for type_key, category in self.EXPLOIT_TYPES.items():
            if type_key in exploit_type:
                return category
        
        # Check for specific vulnerabilities
        if any(kw in title_lower for kw in ['sql injection', 'sqli']):
            return 'web_security/sql_injection'
        if any(kw in title_lower for kw in ['xss', 'cross-site']):
            return 'web_security/xss'
        if any(kw in title_lower for kw in ['rce', 'remote code execution']):
            return 'exploitation/rce'
        if any(kw in title_lower for kw in ['buffer overflow', 'bof']):
            return 'exploitation/buffer_overflow'
        if any(kw in title_lower for kw in ['privilege escalation', 'privesc']):
            return 'privilege_escalation'
        
        return 'exploitation'
    
    def generate_qa_from_item(self, item: ScrapedItem) -> List[Dict[str, Any]]:
        """Generate Q&A pairs from exploit item."""
        qa_pairs = []
        
        cve = item.metadata.get('cve', '')
        platform = item.metadata.get('platform', '')
        exploit_type = item.metadata.get('exploit_type', '')
        
        # Q&A about the exploit
        if cve:
            instruction = f"Explain the vulnerability {cve} and show how to exploit it."
        else:
            instruction = f"Explain the exploit: {item.title}"
        
        output_parts = [item.content]
        
        # Add exploit code
        if item.code_blocks:
            for block in item.code_blocks:
                lang = block.get('language', '')
                code = block.get('code', '')
                output_parts.append(f"\nExploit code:\n```{lang}\n{code}\n```")
        
        qa_pairs.append({
            'instruction': instruction,
            'input': f"Target: {platform}" if platform else '',
            'output': '\n'.join(output_parts)[:4000],
            'category': item.metadata.get('category', 'exploitation'),
            'source': item.url,
            'difficulty': 'advanced',
            'tags': self._generate_tags(item),
        })
        
        return qa_pairs
    
    def _generate_tags(self, item: ScrapedItem) -> List[str]:
        """Generate tags for exploit."""
        tags = ['exploit']
        
        if item.metadata.get('cve'):
            tags.append(item.metadata['cve'].lower())
        
        if item.metadata.get('platform'):
            tags.append(item.metadata['platform'].lower())
        
        if item.metadata.get('exploit_type'):
            tags.append(item.metadata['exploit_type'].lower())
        
        # Extract tools/techniques from title
        title_lower = item.title.lower()
        techniques = ['sqli', 'xss', 'rce', 'lfi', 'rfi', 'bof', 'dos']
        for tech in techniques:
            if tech in title_lower:
                tags.append(tech)
        
        return tags[:10]
