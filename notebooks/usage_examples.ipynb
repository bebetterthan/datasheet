{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aec7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if not already installed)\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# Import scraper components\n",
    "from src.scrapers import HackTricksScraper, OWASPScraper\n",
    "from src.processors import ContentCleaner, FormatConverter, Deduplicator, QualityChecker\n",
    "from src.processors.data_validator import DataValidator\n",
    "from src.processors.data_augmenter import DataAugmenter\n",
    "from src.utils.config import ScrapingConfig\n",
    "from src.generators.qa_generator import QAGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f37bf",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = ScrapingConfig(\n",
    "    delay_between_requests=2.0,\n",
    "    concurrent_requests=3,\n",
    "    timeout=30,\n",
    "    respect_robots_txt=True,\n",
    ")\n",
    "\n",
    "print(f\"Config loaded:\")\n",
    "print(f\"  Delay: {config.delay_between_requests}s\")\n",
    "print(f\"  Concurrent requests: {config.concurrent_requests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8840b40",
   "metadata": {},
   "source": [
    "## 2. Scraping Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scrape HackTricks (limited for demo)\n",
    "async def scrape_demo():\n",
    "    async with HackTricksScraper(config=config) as scraper:\n",
    "        items = []\n",
    "        count = 0\n",
    "        \n",
    "        async for item in scraper.scrape():\n",
    "            items.append(item.to_dict())\n",
    "            count += 1\n",
    "            \n",
    "            if count >= 5:  # Limit for demo\n",
    "                break\n",
    "        \n",
    "        return items\n",
    "\n",
    "# Run the scraper (uncomment to execute)\n",
    "# scraped_items = await scrape_demo()\n",
    "# print(f\"Scraped {len(scraped_items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a2f04",
   "metadata": {},
   "source": [
    "## 3. Content Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a725782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example HTML cleaning\n",
    "cleaner = ContentCleaner(\n",
    "    preserve_code_blocks=True,\n",
    "    preserve_links=True,\n",
    "    normalize_whitespace=True,\n",
    ")\n",
    "\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <nav>Menu items here</nav>\n",
    "    <main>\n",
    "        <h1>SQL Injection Tutorial</h1>\n",
    "        <p>SQL injection is a code injection technique.</p>\n",
    "        <pre><code class=\"language-sql\">SELECT * FROM users WHERE id = '1' OR '1'='1'</code></pre>\n",
    "    </main>\n",
    "    <footer>Footer content</footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "result = cleaner.clean_html(sample_html)\n",
    "print(f\"Title: {result.title}\")\n",
    "print(f\"Text length: {len(result.text)}\")\n",
    "print(f\"Code blocks: {len(result.code_blocks)}\")\n",
    "if result.code_blocks:\n",
    "    print(f\"  Language: {result.code_blocks[0]['language']}\")\n",
    "    print(f\"  Code: {result.code_blocks[0]['code'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bd56b",
   "metadata": {},
   "source": [
    "## 4. Q&A Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130aee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Q&A pairs from content\n",
    "qa_generator = QAGenerator()\n",
    "\n",
    "sample_content = \"\"\"\n",
    "SQL Injection is a type of injection attack that makes it possible to execute malicious SQL statements. \n",
    "These statements control a database server behind a web application.\n",
    "\n",
    "Common techniques include:\n",
    "1. Union-based SQL injection - uses UNION SQL operator\n",
    "2. Error-based SQL injection - forces the database to generate an error\n",
    "3. Blind SQL injection - asks the database true/false questions\n",
    "\n",
    "Example payload:\n",
    "' OR '1'='1' --\n",
    "\n",
    "Prevention:\n",
    "- Use parameterized queries\n",
    "- Input validation\n",
    "- Least privilege principle\n",
    "\"\"\"\n",
    "\n",
    "qa_pairs = qa_generator.generate_from_content(\n",
    "    content=sample_content,\n",
    "    title=\"SQL Injection\",\n",
    "    category=\"web_security\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(qa_pairs)} Q&A pairs:\")\n",
    "for i, qa in enumerate(qa_pairs[:3], 1):\n",
    "    print(f\"\\n--- Q&A #{i} ---\")\n",
    "    print(f\"Q: {qa['instruction'][:100]}...\")\n",
    "    print(f\"A: {qa['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8e14c",
   "metadata": {},
   "source": [
    "## 5. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate samples\n",
    "validator = DataValidator(\n",
    "    min_instruction_length=20,\n",
    "    min_output_length=50,\n",
    ")\n",
    "\n",
    "# Sample data to validate\n",
    "samples = [\n",
    "    {\n",
    "        \"instruction\": \"What is SQL injection and how can it be prevented?\",\n",
    "        \"output\": \"SQL injection is a code injection technique that exploits security vulnerabilities. Prevention includes using parameterized queries and input validation.\",\n",
    "        \"category\": \"web_security\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Short\",  # Too short\n",
    "        \"output\": \"OK\",  # Too short\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain XSS attacks\",\n",
    "        # Missing output\n",
    "    },\n",
    "]\n",
    "\n",
    "report = validator.validate_dataset(samples)\n",
    "\n",
    "print(f\"Validation Report:\")\n",
    "print(f\"  Valid: {report.valid_samples}/{report.total_samples}\")\n",
    "print(f\"  Errors: {report.total_errors}\")\n",
    "print(f\"  Warnings: {report.total_warnings}\")\n",
    "print(f\"  Avg Quality Score: {report.avg_quality_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fef50",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678954a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment dataset\n",
    "augmenter = DataAugmenter()\n",
    "\n",
    "original_sample = {\n",
    "    \"instruction\": \"Explain how to use nmap to scan for open ports\",\n",
    "    \"output\": \"Nmap is a powerful network scanning tool. To scan for open ports, use: nmap -sS target_ip\",\n",
    "    \"category\": \"network_security\",\n",
    "    \"difficulty\": \"beginner\"\n",
    "}\n",
    "\n",
    "augmented = augmenter.augment_sample(original_sample)\n",
    "\n",
    "print(f\"Original: {original_sample['instruction']}\")\n",
    "print(f\"\\nAugmented versions ({len(augmented)}):\")\n",
    "for aug in augmented:\n",
    "    print(f\"  - {aug['instruction'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08202e9",
   "metadata": {},
   "source": [
    "## 7. Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to different formats\n",
    "converter = FormatConverter()\n",
    "\n",
    "# Alpaca format (default)\n",
    "alpaca_sample = converter.to_alpaca(\n",
    "    instruction=\"How to detect SQL injection vulnerabilities?\",\n",
    "    output=\"Use tools like sqlmap, manual testing with payloads, and code review.\",\n",
    "    input_text=\"Given a web application with user input forms\",\n",
    "    category=\"web_security\",\n",
    "    difficulty=\"intermediate\",\n",
    ")\n",
    "\n",
    "print(\"Alpaca Format:\")\n",
    "print(json.dumps(alpaca_sample.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ShareGPT format\n",
    "sharegpt_sample = converter.to_sharegpt(\n",
    "    user_message=\"How do I perform a penetration test on a web application?\",\n",
    "    assistant_message=\"A web application penetration test involves several phases: reconnaissance, scanning, exploitation, and reporting...\",\n",
    "    system_message=\"You are a cybersecurity expert specializing in penetration testing.\",\n",
    ")\n",
    "\n",
    "print(\"ShareGPT Format:\")\n",
    "print(json.dumps(sharegpt_sample.model_dump(by_alias=True), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32d6a0",
   "metadata": {},
   "source": [
    "## 8. Export to Training Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.processors.dataset_exporter import DatasetExporter\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\n",
    "        \"instruction\": \"What is a buffer overflow attack?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A buffer overflow occurs when a program writes more data to a buffer than it can hold...\",\n",
    "        \"category\": \"vulnerability\",\n",
    "        \"difficulty\": \"intermediate\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain cross-site scripting (XSS)\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"XSS is a client-side code injection attack where malicious scripts are injected into web pages...\",\n",
    "        \"category\": \"web_security\",\n",
    "        \"difficulty\": \"beginner\"\n",
    "    },\n",
    "]\n",
    "\n",
    "exporter = DatasetExporter(output_dir=\"../data/exports\")\n",
    "\n",
    "# Export to Axolotl format\n",
    "# files = exporter.export(dataset, format_name=\"axolotl\", filename=\"security_demo\")\n",
    "# print(f\"Exported files: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b763d",
   "metadata": {},
   "source": [
    "## 9. CLI Usage Examples\n",
    "\n",
    "```bash\n",
    "# Scrape all sources\n",
    "security-scraper scrape --all --limit 100\n",
    "\n",
    "# Scrape specific source\n",
    "security-scraper scrape -s hacktricks -s owasp\n",
    "\n",
    "# Process scraped data\n",
    "security-scraper process -i data/raw -o data/processed\n",
    "\n",
    "# Generate dataset\n",
    "security-scraper generate -i data/processed -f alpaca --split\n",
    "\n",
    "# Quality check\n",
    "security-scraper quality -i data/dataset/train.json\n",
    "\n",
    "# Augment dataset\n",
    "security-scraper augment -i data/dataset/train.json -m 2.0\n",
    "\n",
    "# Export for fine-tuning\n",
    "security-scraper export -i data/dataset -f axolotl\n",
    "\n",
    "# Full pipeline\n",
    "security-scraper run --all --limit 100 -f alpaca\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156023e",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3664273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scrapers.base_scraper import BaseScraper, ScrapedItem\n",
    "from typing import AsyncIterator\n",
    "\n",
    "class CustomSecurityScraper(BaseScraper):\n",
    "    \"\"\"Example of creating a custom scraper.\"\"\"\n",
    "    \n",
    "    SOURCE_NAME = \"custom_source\"\n",
    "    BASE_URL = \"https://example-security-site.com\"\n",
    "    \n",
    "    async def scrape(self) -> AsyncIterator[ScrapedItem]:\n",
    "        # Get list of URLs to scrape\n",
    "        urls = await self.get_urls_to_scrape()\n",
    "        \n",
    "        for url in urls:\n",
    "            # Fetch page content\n",
    "            html = await self.fetch_page(url)\n",
    "            if not html:\n",
    "                continue\n",
    "            \n",
    "            # Clean and extract content\n",
    "            content = self.content_cleaner.clean_html(html)\n",
    "            \n",
    "            # Create scraped item\n",
    "            yield ScrapedItem(\n",
    "                url=url,\n",
    "                title=content.title,\n",
    "                content=content.text,\n",
    "                code_blocks=content.code_blocks,\n",
    "                headers=content.headers,\n",
    "                metadata={'source': self.SOURCE_NAME}\n",
    "            )\n",
    "    \n",
    "    async def get_urls_to_scrape(self):\n",
    "        \"\"\"Override to provide URLs.\"\"\"\n",
    "        return []\n",
    "\n",
    "print(\"Custom scraper class defined successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
