# ğŸš€ Quick Start: Testing LLM Provider

## Prerequisites

1. **Install dependencies:**
```bash
pip install -r requirements.txt
```

## Testing Different Providers

### Option 1: API Provider (Recommended for Testing)

**Using Ollama (Local):**
```bash
# 1. Install Ollama: https://ollama.ai
# 2. Pull a model
ollama pull llama2

# 3. Test the provider
cd redteam_agent
python scripts/test_llm.py --provider api --api-url http://localhost:11434/v1 --model-name llama2
```

**Using vLLM Server:**
```bash
# 1. Start vLLM server (in another terminal)
vllm serve Qwen/Qwen2.5-Coder-7B-Instruct --port 8000

# 2. Test the provider
python scripts/test_llm.py --provider api --api-url http://localhost:8000/v1
```

**Using LM Studio:**
```bash
# 1. Start LM Studio and load a model
# 2. Enable API server (usually on port 1234)
# 3. Test the provider
python scripts/test_llm.py --provider api --api-url http://localhost:1234/v1
```

### Option 2: OpenAI Provider

```bash
# Set API key
export OPENAI_API_KEY="sk-..."

# Test with GPT-4
python scripts/test_llm.py --provider openai --model-name gpt-4

# Test with GPT-3.5 (cheaper)
python scripts/test_llm.py --provider openai --model-name gpt-3.5-turbo
```

### Option 3: Local Provider (Requires GPU)

```bash
# This loads the model directly on your machine
# Requires: 16GB+ GPU RAM for 7B model
python scripts/test_llm.py --provider local
```

## Expected Output

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸ¤– Red Team Agent - LLM Provider Test      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Creating api provider...
âœ… Provider created: APILLMProvider

ğŸ“‹ Provider Information:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Property     â”‚ Value                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ provider_typeâ”‚ APILLMProvider              â”‚
â”‚ healthy      â”‚ True                        â”‚
â”‚ api_url      â”‚ http://localhost:8000/v1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¥ Health Check:
âœ… Provider is healthy

ğŸ§ª Testing Simple Generation:
â ¹ Generating response...
â•­â”€ Generation Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Prompt:                                      â”‚
â”‚ What is a SQL injection vulnerability?      â”‚
â”‚                                              â”‚
â”‚ Response:                                    â”‚
â”‚ SQL injection is a security vulnerability... â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

...

ğŸ“Š Test Summary:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test             â”‚ Result â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Provider Info    â”‚ âœ… PASSâ”‚
â”‚ Health Check     â”‚ âœ… PASSâ”‚
â”‚ Simple Generationâ”‚ âœ… PASSâ”‚
â”‚ Chat Generation  â”‚ âœ… PASSâ”‚
â”‚ Security Task    â”‚ âœ… PASSâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Results: 5/5 tests passed
ğŸ‰ All tests passed! Provider is ready to use.
```

## Configuration

Edit `configs/agent_config.yaml` to set default provider:

```yaml
llm:
  provider: "api"  # Change to: local, api, or openai
  api_url: "http://localhost:8000/v1"
  model_name: "your-model-name"
  max_tokens: 4096
  temperature: 0.7
```

## Troubleshooting

### "Could not connect to API"
- Check if the API server is running
- Verify the URL is correct
- Test with: `curl http://localhost:8000/v1/models`

### "API key is required"
- Set environment variable: `export OPENAI_API_KEY="sk-..."`
- Or add to config: `api_key: "your-key"`

### "torch is required"
- Install PyTorch: `pip install torch transformers peft`
- For local provider only

### "Model not found"
- Check model name matches what's available
- For Ollama: `ollama list`
- For vLLM: Check startup logs

## Next Steps

Once tests pass, you can:

1. **Test the full agent:**
   ```bash
   python scripts/run_agent.py run "Test query"
   ```

2. **Interactive mode:**
   ```bash
   python scripts/interactive.py
   ```

3. **API server:**
   ```bash
   python scripts/serve_api.py
   ```

## Provider Comparison

| Provider | Pros | Cons |
|----------|------|------|
| **API (Ollama)** | âœ… Easy setup<br>âœ… Free<br>âœ… No GPU needed | âš ï¸ Slower inference<br>âš ï¸ Limited model quality |
| **API (vLLM)** | âœ… Fast inference<br>âœ… Free | âš ï¸ Requires GPU<br>âš ï¸ Setup complexity |
| **OpenAI** | âœ… Best quality<br>âœ… No setup | âŒ Costs money<br>âŒ Requires internet |
| **Local** | âœ… Private<br>âœ… Fast (with GPU) | âŒ Requires 16GB+ GPU<br>âŒ Complex setup |

**Recommended for Development:** Start with Ollama (easiest) or OpenAI (best quality)
**Recommended for Production:** vLLM server or Local (after fine-tuning)
