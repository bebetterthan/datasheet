# ğŸ›¡ï¸ Security Dataset Scraper

**Comprehensive tool for collecting security/pentesting datasets for LLM fine-tuning**

Build high-quality training datasets for Red Team AI agents by scraping and processing security knowledge from multiple authoritative sources.

## âœ¨ Features

### Core Features

- **Multi-Source Scraping**: HackTricks, CTFTime, Exploit-DB, NVD/CVE, Nuclei Templates, PayloadsAllTheThings, OWASP
- **Intelligent Processing**: Deduplication, quality filtering, automatic categorization
- **Flexible Output**: Alpaca, ShareGPT, OpenAI, Axolotl, LLaMA-Factory, Unsloth formats
- **Q&A Generation**: Automatic question-answer pair generation from content
- **Resume Capability**: SQLite-based progress tracking for interrupted scrapes
- **Rate Limiting**: Adaptive rate limiting with proxy rotation support
- **Docker Support**: Easy deployment with Docker and docker-compose

### Advanced Features (NEW)

- **ğŸ”„ Data Augmentation**: Automatic paraphrasing, context variation, difficulty scaling
- **âœ… Data Validation**: Comprehensive quality checks with detailed reports
- **ğŸ“Š Analytics**: Token counting, category distribution, quality scoring
- **ğŸ’¾ Disk Caching**: TTL-based caching with compression
- **ğŸ” Circuit Breaker**: Robust retry logic with exponential backoff
- **ğŸ“¦ Batch Processing**: Parallel processing with checkpointing
- **ğŸ“ˆ Streaming Export**: Memory-efficient export for large datasets

## ğŸ“Š Target Output

- **10,000+ Q&A pairs** covering security topics
- **13+ security categories**: Web Security, Exploitation, Privilege Escalation, Credential Access, and more
- **Multiple formats** ready for fine-tuning with Axolotl, LLaMA-Factory, Unsloth

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/your-repo/security-dataset-scraper.git
cd security-dataset-scraper

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -e ".[dev]"

# Install Playwright browsers (for JS-rendered pages)
playwright install chromium
```

### Quick Start with Make

```bash
# Setup everything
make quickstart

# Run full pipeline
make scrape-all
make process
make export-axolotl
```

### Basic Usage

```bash
# List available sources
python main.py list

# Scrape from specific sources
python main.py scrape --source hacktricks --source owasp --limit 100

# Scrape all sources
python main.py scrape --all --limit 500

# Process scraped data
python main.py process -i data/raw -o data/processed

# Generate training dataset
python main.py generate -i data/processed -o data/dataset -f alpaca

# Quality check
python main.py quality -i data/dataset/train.json --strict

# Augment dataset
python main.py augment -i data/dataset/train.json -m 2.0

# Export to different formats
python main.py export -i data/dataset -f axolotl
python main.py export -i data/dataset -f llama_factory

# Analyze dataset
python main.py analyze -i data/dataset/train.json -o report.md

# Run full pipeline
python main.py run --all --limit 100

# View statistics
python main.py stats -i data/dataset --detailed
```

### Docker Usage

```bash
# Build and run
docker compose up --build

# Run specific command
docker compose run scraper scrape --source hacktricks --limit 50

# Run full pipeline
docker compose run scraper run --all --limit 100

# With Jupyter for analysis
docker compose --profile analysis up
```

## ğŸ“ Project Structure

```
datasheet_scraper/
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pyproject.toml             # Project configuration
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ Makefile                   # Automation commands
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ docker-compose.yml         # Docker Compose configuration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml           # Main configuration
â”‚   â””â”€â”€ sources.yaml          # Source-specific configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/             # Source-specific scrapers
â”‚   â”‚   â”œâ”€â”€ base_scraper.py   # Base scraper class
â”‚   â”‚   â”œâ”€â”€ hacktricks_scraper.py
â”‚   â”‚   â”œâ”€â”€ ctf_writeup_scraper.py
â”‚   â”‚   â”œâ”€â”€ exploit_db_scraper.py
â”‚   â”‚   â”œâ”€â”€ cve_scraper.py
â”‚   â”‚   â”œâ”€â”€ nuclei_templates_scraper.py
â”‚   â”‚   â”œâ”€â”€ payloads_scraper.py
â”‚   â”‚   â””â”€â”€ owasp_scraper.py
â”‚   â”œâ”€â”€ processors/           # Data processing modules
â”‚   â”‚   â”œâ”€â”€ content_cleaner.py
â”‚   â”‚   â”œâ”€â”€ format_converter.py
â”‚   â”‚   â”œâ”€â”€ deduplicator.py
â”‚   â”‚   â”œâ”€â”€ quality_checker.py
â”‚   â”‚   â”œâ”€â”€ category_classifier.py
â”‚   â”‚   â”œâ”€â”€ dataset_exporter.py   # NEW: Multi-format export
â”‚   â”‚   â”œâ”€â”€ batch_processor.py    # NEW: Parallel batch processing
â”‚   â”‚   â”œâ”€â”€ data_validator.py     # NEW: Quality validation
â”‚   â”‚   â””â”€â”€ data_augmenter.py     # NEW: Data augmentation
â”‚   â”œâ”€â”€ generators/           # Q&A generation
â”‚   â”‚   â””â”€â”€ qa_generator.py
â”‚   â””â”€â”€ utils/                # Utilities
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ rate_limiter.py
â”‚       â”œâ”€â”€ proxy_manager.py
â”‚       â”œâ”€â”€ cache.py          # NEW: Disk caching
â”‚       â”œâ”€â”€ retry.py          # NEW: Circuit breaker
â”‚       â””â”€â”€ analytics.py      # NEW: Dataset analytics
â”œâ”€â”€ data/                     # Output data
â”‚   â”œâ”€â”€ raw/                  # Raw scraped data
â”‚   â”œâ”€â”€ processed/            # Processed data
â”‚   â”œâ”€â”€ dataset/              # Final dataset
â”‚   â”œâ”€â”€ exports/              # Exported formats
â”‚   â””â”€â”€ checkpoints/          # Processing checkpoints
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â””â”€â”€ usage_examples.ipynb  # Usage examples
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â””â”€â”€ init_db.sql          # PostgreSQL init
â””â”€â”€ tests/                    # Unit tests
```

## ğŸ“¦ Data Sources

| Source                   | Description                            | Content Type               |
| ------------------------ | -------------------------------------- | -------------------------- |
| **HackTricks**           | Comprehensive pentesting documentation | Methodology, techniques    |
| **CTFTime**              | CTF competition writeups               | Challenges, solutions      |
| **Exploit-DB**           | Public exploits database               | Exploit code, advisories   |
| **NVD/CVE**              | Vulnerability database                 | CVE details, CVSS scores   |
| **Nuclei Templates**     | Security detection templates           | YAML templates             |
| **PayloadsAllTheThings** | Security payloads collection           | Payloads, techniques       |
| **OWASP**                | Security best practices                | CheatSheets, Testing Guide |

## ğŸ¯ Security Categories

The dataset covers these security domains:

- **Reconnaissance**: Information gathering, network scanning
- **Web Security**: SQL injection, XSS, CSRF, SSRF, etc.
- **Exploitation**: Buffer overflow, RCE, code injection
- **Privilege Escalation**: Linux/Windows privesc techniques
- **Credential Access**: Password attacks, token theft
- **Lateral Movement**: Active Directory, pivoting
- **Defense Evasion**: AV bypass, obfuscation
- **Persistence**: Backdoors, rootkits
- **Cryptography**: Encryption, hashing, PKI

## ğŸ“„ Output Format

### Alpaca Format (Default)

```json
{
  "instruction": "How do I perform SQL injection on a login form?",
  "input": "",
  "output": "SQL injection on login forms can be performed by...",
  "category": "web_security/sql_injection",
  "source": "https://book.hacktricks.xyz/...",
  "difficulty": "intermediate",
  "tags": ["sqli", "web", "authentication"]
}
```

### ShareGPT Format

```json
{
  "conversations": [
    { "from": "human", "value": "How do I perform SQL injection?" },
    { "from": "gpt", "value": "SQL injection can be performed by..." }
  ]
}
```

## âš™ï¸ Configuration

Edit `config/config.yaml` to customize:

```yaml
scraping:
  timeout: 30
  max_retries: 3
  concurrent_requests: 10

rate_limiting:
  requests_per_second: 2.0
  adaptive: true

processing:
  min_quality_score: 0.5
  deduplicate: true

output:
  format: "alpaca"
  split_dataset: true
  train_ratio: 0.8
```

## ğŸ”§ CLI Commands

| Command    | Description                     |
| ---------- | ------------------------------- |
| `scrape`   | Scrape content from sources     |
| `process`  | Process and clean scraped data  |
| `generate` | Generate training dataset       |
| `run`      | Run full pipeline               |
| `stats`    | Show dataset statistics         |
| `validate` | Validate dataset format         |
| `quality`  | Run comprehensive quality check |
| `augment`  | Augment dataset with variations |
| `export`   | Export to fine-tuning formats   |
| `analyze`  | Generate analytics report       |
| `clean`    | Clean cache and temp files      |
| `list`     | List available sources          |

### Common Options

- `--verbose, -v`: Enable verbose output
- `--config, -c`: Specify config file
- `--limit, -l`: Limit items per source
- `--resume`: Resume from checkpoint
- `--dry-run`: Preview without scraping

## ğŸ›ï¸ Export Formats

| Format          | Description                     | Use With                  |
| --------------- | ------------------------------- | ------------------------- |
| `alpaca`        | Standard instruction format     | Most fine-tuning tools    |
| `sharegpt`      | Conversation format             | ShareGPT-compatible tools |
| `openai`        | OpenAI chat format              | OpenAI API fine-tuning    |
| `axolotl`       | Axolotl YAML config + dataset   | Axolotl trainer           |
| `llama_factory` | LLaMA-Factory compatible        | LLaMA-Factory             |
| `unsloth`       | Unsloth format with HF datasets | Unsloth trainer           |

## ğŸ› ï¸ Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/ -v

# Run tests with coverage
make test-cov

# Code formatting
make format

# Linting
make lint

# All checks
make check-all
```

## âš ï¸ Legal & Ethical Use

This tool is intended for **educational and research purposes only**. Users must:

- Respect robots.txt and rate limits
- Comply with each source's Terms of Service
- Use collected data responsibly
- Not use for malicious purposes

## ğŸ“ License

MIT License - See LICENSE file for details.

## ğŸ¤ Contributing

Contributions welcome! Please read CONTRIBUTING.md for guidelines.

---

**Built for Red Team AI Research** ğŸ”´
